---
title: "[Paper Review] VGGT: Visual Geometry Grounded Transformer"
date: 2025-10-17
categories:
  - paper review
tags:
  - Scene representation
  - View synthesis
  - Neural Rendering
  - Transformer
  - 3D Vision
  - Multi-view Geometry
  - SfM
  - Depth Esitmation
  - Point Tracking
use_math: true
classes: wide
---

![VGGT]({{ site.baseurl }}/assets/VGGT/VGGT_main.png)


## Introduction

In this post, I review **VGGT: Visual Geometry Grounded Transformer** by **Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny**. VGGT is a **large feed-forward transformer** with **minimal 3D inductive biases** that ingests **one to hundreds of images** and, **in a single forward pass**, predicts **camera intrinsics/extrinsics, depth maps, viewpoint-invariant point maps, and dense tracking features**. Despite being purely feed-forward, it **often outperforms optimization-based** pipelines; a brief BA refinement can push it even further. The model also serves as a **general-purpose backbone** that boosts downstream **point tracking** and **feed-forward novel view synthesis (NVS)**.

## Paper Info

- **Title**: VGGT: Visual Geometry Grounded Transformer  
- **Authors**: Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny  
- **Affiliations**: Visual Geometry Group (University of Oxford), Meta AI  
- **Conference**: CVPR 2026
- **Code**: [VGGT](https://github.com/facebookresearch/vggt)


## Background: Geometry Pipelines vs. Feed-Forward Models

Traditional **SfM/MVS** stacks (matching → triangulation → BA) deliver accurate geometry but require **iterative optimization** and careful engineering. Recent neural methods (e.g., **DUSt3R/MASt3R**) move toward **feed-forward 3D prediction**, yet are typically **pairwise** and rely on **post-alignment** for multi-frame scenes.  
**VGGT** advances this line: **multi-view, all-at-once** predictions with **no explicit 3D representation** or heavy geometric modules, trained on a **trove of 3D-annotated data**.

---

## VGGT

![VGGT]({{ site.baseurl }}/assets/VGGT/VGGT_explain.png)

### 1. Problem Formulation

Given images $$\{I_i\}_{i=1}^N$$ of a scene, the transformer predicts for each frame:

$$
f\big(\{I_i\}_{i=1}^N\big)=\{(g_i, D_i, P_i, T_i)\}_{i=1}^N,
$$

where:
- $$g_i=[q_i, t_i, f_i]$$ are **camera** params (quaternion, translation, FoV; principal point fixed at center),
- $$D_i\in\mathbb{R}^{H\times W}$$ are **depth** maps,
- $$P_i\in\mathbb{R}^{3\times H\times W}$$ are **point maps** expressed in the **first camera’s** coordinate frame (viewpoint-invariant),
- $$T_i\in\mathbb{R}^{C\times H\times W}$$ are **dense features** for tracking.

**Reference frame**: the first image is special (identity extrinsics); all 3D outputs live in that frame.

---

### 2. Backbone: Alternating-Attention (AA)

- **Tokenization**: each image is patchified by **DINO**, then augmented with a **camera token** and **register tokens**.  
- **AA stack (L layers)**: the network **alternates**
  - **Frame-wise self-attention** (within each image), and
  - **Global self-attention** (across all images).  
This balances per-frame normalization and multi-view fusion. Notably, **no cross-attention** is used, and the design is **permutation-equivariant** except for the first (reference) frame.

---

### 3. Heads & Predictions

- **Camera head**: additional self-attention + linear layer → $$\hat g_i$$.
- **Dense head (DPT)**: image tokens → dense $$F_i$$ → conv → $$\hat D_i$$, $$\hat P_i$$ and **tracking features** $$T_i$$.
- **Uncertainty**: predicts $$\Sigma^D_i,\Sigma^P_i$$(aleatoric) to weight losses/confidence.
- **Tracking**: a **CoTracker-style** module consumes $$\{T_i\}$$ and query points to output **2D correspondences** across frames; trained **end-to-end** with the backbone.

**Over-complete supervision matters**: while $$P$$ can be derived from $$(D,g)$$ and $$g$$ from $$P$$ (PnP), **jointly** supervising **all** improves accuracy. At inference, **unprojecting depth with predicted cameras** yields **cleaner point clouds** than the direct point-map head.

---

## Training

Supervision uses large-scale **3D-annotated datasets** (posed images, multi-view depth/points, correspondences). Losses include:
- **Photometric / geometric** terms for depth & points (uncertainty-weighted),
- **Camera regression** losses,
- **Tracking** losses on correspondences/visibility.

Practical notes:
- **Batch many frames**; AA scales to **hundreds** of views.
- Keep the **first frame fixed** per batch to stabilize the global frame.

---

## Results

![VGGT]({{ site.baseurl }}/assets/VGGT/VGGT_results.png)

**Camera Pose (10 frames)** — **CO3Dv2** & **RealEstate10K**  
- **VGGT (feed-forward)**: **SOTA** AUC@30 while running in **~0.2s** (H100).  
- **VGGT + BA (~1.8s)**: pushes accuracy further, surpassing optimization-heavy baselines.

**Multi-View Depth (DTU)**  
- Without using GT cameras at test time, VGGT attains **Chamfer ≈ 0.38**, a **large margin** over DUSt3R and competitive with methods that **assume** known cameras.

**Point Maps (ETH3D)**  
- **Feed-forward** (no global alignment) yet **more accurate** and **faster** than DUSt3R/MASt3R with alignment.  
- Best results via **Depth + Cam → Points** at inference.

**Two-View Matching (ScanNet-1500)**  
- Using ALIKED keypoints + VGGT tracker, achieves **higher AUC** than specialized two-view matcher **Roma**.

**Downstream: NVS (GSO)**  
- A **camera-free** variant (inputs have unknown cameras; target views via Plücker rays) achieves **competitive PSNR/SSIM/LPIPS** compared to camera-aware LVMs, **with less training data**.

**Downstream: Dynamic Point Tracking (TAP-Vid)**  
- Swapping CoTracker’s backbone with **VGGT features** and finetuning yields **consistent gains** in **AJ**, **δ\_{avg}^{vis}**, and **OA** across Kinetics/RGB-S/DAVIS.

**Ablations**  
- **Alternating-Attention >** global-only or cross-attention variants.  
- **Multi-task supervision** (camera + depth + track) **strictly improves** point-map accuracy vs. removing any head.

---

## Limitations

- **Residual pose/depth drift** in extremely challenging or very long sequences; a brief **BA refinement** is still beneficial.  
- **No explicit 3D inductive bias** may underperform in settings where strong geometric priors help with limited data.  
- **First-frame anchoring** requires care in batching (keep reference consistent).  
- As a **large transformer**, training demands substantial **data and compute**.

---

## Takeaways

VGGT shows that a **simple, neural-first, feed-forward** transformer—scaled and trained on **rich 3D supervision**—can **replace much of** the classical geometry stack while being **fast**, **accurate**, and **versatile**. For practitioners, two tips stand out:

1. **Use VGGT as a backbone** for **tracking** and **NVS**—it transfers well.  
2. At inference, prefer **Depth + Cam → Points** for the cleanest reconstructions, and add **quick BA** if you need extra polish.

**Repo**: <https://github.com/facebookresearch/vggt>
