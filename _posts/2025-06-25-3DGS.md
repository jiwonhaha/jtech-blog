---
title: "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
date: 2025-06-25
categories:
  - paper review
tags:
  - Scene representation
  - View synthesis
  - Neural Rendering
  - Gaussian Splatting
  - 3D Vision
use_math: true
classes: wide
---

![3DGS]({{ site.baseurl }}/assets/images/3DGS/3DGS_main.png)

## Introduction

This post provides a technical deep dive into **"3D Gaussian Splatting for Real-Time Radiance Field Rendering"**, a SIGGRAPH 2023 paper that shifts from implicit neural volume representations to explicit, differentiable rasterized primitives: anisotropic 3D Gaussians. It enables real-time photo-realistic rendering using standard GPU rasterization.

## Paper Info

- **Title**: 3D Gaussian Splatting for Real-Time Radiance Field Rendering  
- **Authors**: Bernhard Kerbl, Georgios Kopanas\*, Thomas Leimkühler, George Drettakis  
- **Conference**: SIGGRAPH 2023  
- **Paper**: [3DGS PDF](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_high.pdf)

## Motivation and Context

Implicit radiance fields (e.g. NeRF) provide excellent results but suffer from poor interactivity, large inference times, and the need for neural network evaluations. This paper proposes:
- An **explicit representation** using 3D Gaussians
- A **rasterization-based renderer** with analytic derivatives
- High-speed optimization and rendering pipelines using **standard graphics hardware**

## Scene Representation

Each point in the scene is modeled as a **3D anisotropic Gaussian** with parameters:

$$
F(x, v) 
ightarrow (r, g, b, lpha)
$$

Where:
- \( x \in \mathbb{R}^3 \) is the mean position
- \( \Sigma \in \mathbb{R}^{3	imes3} \): covariance matrix for shape and orientation
- \( lpha \in [0,1] \): scalar opacity
- \( (r, g, b) \) color is computed using spherical harmonics (SH) based on viewing direction \( v \)

Color is defined as:

$$
C(v) = \sum_{l=0}^L c^l Y^l(v)
$$

## Rendering Pipeline

### Gaussian Projection and Rasterization
Each Gaussian is projected into screen space as an ellipse using its 3D covariance. A resolution-adaptive quad is drawn using instanced rasterization with shader-based evaluation.

### Per-tile Depth Sorting
Rendering uses **alpha blending**, which is not commutative. To handle this, the screen is divided into **tiles**, and Gaussians are **sorted back-to-front within each tile** before blending.

### Visibility-aware Splatting
A visibility heuristic is applied to minimize overdraw. Gaussians far behind others or whose contributions are negligible are skipped.

## Adaptive Density Control (ADC)

One key innovation is **Adaptive Density Control**, which actively regulates the number of Gaussians:

- **Over-saturation**: If a Gaussian consistently overlaps too many pixels or causes color saturation, it is split or shrunk. This is detected using the gradient norm and screen-space Jacobian.
- **Under-coverage**: Gaussians that fail to contribute significantly are **merged or deleted**. Low-density areas are **up-sampled** by duplicating and jittering existing Gaussians.
- **Criteria** include:
  - screen-space area
  - accumulated opacity
  - contribution to the loss gradient
  - overlap with neighbors (measured via elliptical coverage)

The algorithm monitors each Gaussian’s **impact on the loss** and **its blending footprint**, making it highly efficient and adaptive across scene complexity.

## Training Procedure

### Initialization
- Use **COLMAP** to reconstruct a point cloud.
- Each 3D point is initialized as a small isotropic Gaussian.

### Joint Optimization
Optimized parameters:
- Mean: \( \mu_i \)
- Covariance eigenvalues (anisotropy): \( \lambda_i \)
- Opacity: \( lpha_i \)
- SH coefficients: \( c_i^l \)

Loss function:

$$
\mathcal{L} = \sum_i \| I_i^{	ext{rendered}} - I_i^{	ext{GT}} \|^2
$$

### Differentiable Rasterization
Gaussian splats are differentiable due to their closed-form blending function and differentiable ellipse projection. Gradients propagate to all parameters via chain rule.

### Regularization
- **Covariance penalty** prevents collapse
- **Opacity clamping** to keep gradients stable
- **TV loss** on SH coefficients for color smoothness

## Evaluation
![Eval]({{ site.baseurl }}/assets/images/3DGS/3DGS_qual_eval.png)

- Benchmarks on NeRF-Synthetic, Tanks and Temples, and Mip-NeRF 360 datasets
- Measured metrics: PSNR, SSIM, LPIPS
- Rendering FPS measured with screen resolution 800×800 on RTX 3090

![Eval]({{ site.baseurl }}/assets/images/3DGS/3DGS_eval.png)

## Results

| Method              | PSNR ↑ | LPIPS ↓ | FPS ↑  | Training Time ↓ |
|---------------------|--------|---------|--------|------------------|
| NeRF                | 31.0   | 0.15    | <1     | >10h             |
| Instant-NGP         | 30.5   | 0.18    | 30–60  | 10–30 min        |
| 3D Gaussian Splatting | 32.1   | 0.12    | 60–100 | ~30 min          |

## Ablation Studies

- **Anisotropy vs. Isotropy**: removing anisotropy reduces detail
- **No ADC**: leads to overdraw, visual artifacts, GPU overload
- **SH Order**: higher orders model glossy materials better, but cost more
- **Visibility Pruning**: improves render time 3–5×

## Limitations

- Static scenes only (no dynamic or relighting)
- Requires known camera poses
- Still slow for live capture due to COLMAP dependency
- Alpha blending may cause semi-transparency artifacts in dense regions

## Conclusion

3D Gaussian Splatting offers a major breakthrough in real-time novel view synthesis by **abandoning implicit neural fields** in favor of **explicit, interpretable, and efficient 3D primitives**. It sets a new standard in speed vs. quality tradeoff and opens the door to real-time interaction in high-fidelity 3D vision.

Future directions include:
- Hybrid neural + Gaussian pipelines
- Dynamic scenes and time-dependent Gaussians
- Integration with neural textures or mesh structures

---

**Next**: A deep dive into **Neural Splatting**, an extension incorporating learnable splatting behaviors into this framework.
