---
title: "3D Gaussian Splatting for Real-Time Radiance Field Rendering"
date: 2025-06-25
categories:
  - paper review
tags:
  - Scene representation
  - View synthesis
  - Neural Rendering
  - Gaussian Splatting
  - 3D Vision
use_math: true
classes: wide
---

![3DGS]({{ site.baseurl }}/assets/3DGS/3DGS_main.png)

## Introduction

This post provides a technical deep dive into **"3D Gaussian Splatting for Real-Time Radiance Field Rendering"**, a SIGGRAPH 2023 paper that shifts from implicit neural volume representations to explicit, differentiable rasterized primitives: anisotropic 3D Gaussians. It enables real-time photo-realistic rendering using standard GPU rasterization.

## Paper Info

- **Title**: 3D Gaussian Splatting for Real-Time Radiance Field Rendering  
- **Authors**: Bernhard Kerbl, Georgios Kopanas\*, Thomas Leimkühler, George Drettakis  
- **Conference**: SIGGRAPH 2023  
- **Paper**: [3DGS PDF](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/3d_gaussian_splatting_high.pdf)

## Motivation and Context

Implicit radiance fields (e.g. NeRF) provide excellent results but suffer from poor interactivity, large inference times, and the need for neural network evaluations. This paper proposes:
- An **explicit representation** using 3D Gaussians
- A **rasterization-based renderer** with analytic derivatives
- High-speed optimization and rendering pipelines using **standard graphics hardware**

## Scene Representation

Each point in the scene is modeled as a **3D anisotropic Gaussian** with parameters:

$$
F(x, v) 
\rightarrow (r, g, b, \alpha)
$$

Where:
- $$x \in \mathbb{R}^3$$ is the mean position
- $$\Sigma \in \mathbb{R}^{3 \times 3}$$: full **covariance matrix** encoding the **shape** (scaling along principal axes) and **orientation** (rotation) of the 3D Gaussian in space.  
  *In practice (e.g., in the code), this is often represented more compactly as a 3×1 vector of eigenvalues (scales) and a quaternion for rotation, from which the full covariance matrix is reconstructed.*
- $$\alpha \in [0,1]$$: scalar opacity
- $$(r, g, b)$$ color is computed using spherical harmonics (SH) based on viewing direction (v)

Color is defined as:

$$
C(v) = \sum_{l=0}^L c^l Y^l(v)
$$

## Rendering Pipeline

### Gaussian Projection and Rasterization
Each Gaussian is projected into screen space as an ellipse using its 3D covariance. A resolution-adaptive quad is drawn using instanced rasterization with shader-based evaluation.

### Per-tile Depth Sorting
Rendering uses **alpha blending**, which is not commutative. To handle this, the screen is divided into **tiles**, and Gaussians are **sorted back-to-front within each tile** before blending.

### Visibility-aware Splatting
A visibility heuristic is applied to minimize overdraw. Gaussians far behind others or whose contributions are negligible are skipped.

## Adaptive Density Control (ADC)

One key innovation is **Adaptive Density Control**, which actively regulates the number of Gaussians:

- **Over-saturation**: If a Gaussian consistently overlaps too many pixels or causes color saturation, it is split or shrunk. This is detected using the gradient norm and screen-space Jacobian.
- **Under-coverage**: Gaussians that fail to contribute significantly are **merged or deleted**. Low-density areas are **up-sampled** by duplicating and jittering existing Gaussians.
- **Criteria** include:
  - screen-space area
  - accumulated opacity
  - contribution to the loss gradient
  - overlap with neighbors (measured via elliptical coverage)

The algorithm monitors each Gaussian’s **impact on the loss** and **its blending footprint**, making it highly efficient and adaptive across scene complexity.

## Training Procedure

### Initialization

Training begins with a sparse point cloud reconstructed using **COLMAP**, which provides a 3D structure from multi-view images. Each point is initialized as a **small isotropic Gaussian**, with the initial covariance estimated as the mean distance to its three nearest neighbors. This gives a reasonable initial footprint for each Gaussian in 3D space.

### Optimization Strategy

Training proceeds via **iterative render-and-compare cycles**. In each step:
- The current set of Gaussians is rendered to an image.
- The result is compared to ground truth views.
- Gradients are backpropagated to update Gaussian parameters.

Because projecting 3D geometry into 2D images is inherently ambiguous, the training must dynamically **create, delete, and move Gaussians**. This allows it to correct misalignments and converge toward a compact, accurate representation.

Optimization is performed using **Stochastic Gradient Descent (SGD)** within a GPU-accelerated framework. Critical steps like rasterization use **custom CUDA kernels**, enabling real-time training performance.

### Activation Functions

To ensure valid parameter ranges and smooth gradients:
- **Opacity** ($\alpha$) is passed through a **sigmoid** to constrain it to $[0, 1)$.
- **Covariance scaling** is passed through an **exponential** function to maintain positive definiteness and stable optimization.

### Loss Function

The loss is a **weighted combination of L1 loss and D-SSIM** (Differentiable Structural Similarity):

$$
\mathcal{L} = (1 - \lambda) \cdot \mathcal{L}_{\text{L1}} + \lambda \cdot \mathcal{L}_{\text{D-SSIM}}
$$

- **L1 loss** penalizes per-pixel absolute differences:
  
  $$
  \mathcal{L}_{\text{L1}} = \sum_{i} \left| I_i^{\text{rendered}} - I_i^{\text{GT}} \right|
  $$

- **D-SSIM** captures perceptual similarity by comparing local structures rather than raw pixel values. It is a differentiable variant of the SSIM metric, enabling effective supervision that preserves edges and textures.

The hyperparameter $$\lambda$$ balances low-level accuracy and perceptual quality.

### Learning Rate Schedule

A **decaying learning rate** is applied to the position updates, following the scheduling strategy used in **Plenoxels**. This improves stability and helps avoid overshooting as training progresses.

### Differentiable Rasterization

The rendering process is fully differentiable. Each Gaussian is projected to an anisotropic 2D ellipse and blended in screen space. This projection and blending are implemented with **efficient CUDA-based rasterization**, which is the main computational bottleneck but remains fast enough for real-time training updates.


### Regularization
- **Covariance penalty** prevents collapse
- **Opacity clamping** to keep gradients stable
- **TV loss** on SH coefficients for color smoothness

## Evaluation
![Eval]({{ site.baseurl }}/assets/3DGS/3DGS_qual_eval.png)

- Benchmarks on NeRF-Synthetic, Tanks and Temples, and Mip-NeRF 360 datasets
- Measured metrics: PSNR, SSIM, LPIPS
- Rendering FPS measured with screen resolution 800×800 on RTX 3090

![Eval]({{ site.baseurl }}/assets/3DGS/3DGS_eval.png)

## Results

| Method              | PSNR ↑ | LPIPS ↓ | FPS ↑  | Training Time ↓ |
|---------------------|--------|---------|--------|------------------|
| NeRF                | 31.0   | 0.15    | <1     | >10h             |
| Instant-NGP         | 30.5   | 0.18    | 30–60  | 10–30 min        |
| 3D Gaussian Splatting | 32.1   | 0.12    | 60–100 | ~30 min          |

## Ablation Studies

- **Anisotropy vs. Isotropy**: removing anisotropy reduces detail
- **No ADC**: leads to overdraw, visual artifacts, GPU overload
- **SH Order**: higher orders model glossy materials better, but cost more
- **Visibility Pruning**: improves render time 3–5×

## Limitations

- Static scenes only (no dynamic or relighting)
- Requires known camera poses
- Still slow for live capture due to COLMAP dependency
- Alpha blending may cause semi-transparency artifacts in dense regions

## Conclusion

3D Gaussian Splatting offers a major breakthrough in real-time novel view synthesis by **abandoning implicit neural fields** in favor of **explicit, interpretable, and efficient 3D primitives**. It sets a new standard in speed vs. quality tradeoff and opens the door to real-time interaction in high-fidelity 3D vision.

Future directions include:
- Hybrid neural + Gaussian pipelines
- Dynamic scenes and time-dependent Gaussians
- Integration with neural textures or mesh structures

---

**Next**: A deep dive into **Neural Splatting**, an extension incorporating learnable splatting behaviors into this framework.
