---
title: "2D Gaussian Splatting for Geometrically Accurate Radiance Fields"
date: 2025-07-09
categories:
  - paper review
tags:
  - Scene representation
  - View synthesis
  - Neural Rendering
  - Gaussian Splatting
  - 3D Vision
use_math: true
classes: wide
---
![2DGS]({{ site.baseurl }}/assets/images/2DGS/2DGS_main.png)

## Introduction

In this post, I review **2D Gaussian Splatting for Geometrically Accurate Radiance Fields**, a recent paper presented at CVPR 2024 by **Binbin Huang et al.** This work builds on the success of **3D Gaussian Splatting (3DGS)** and introduces a novel approach for accurately modeling geometry and appearance using **planar 2D Gaussian primitives** embedded in 3D space. Unlike 3DGS, which struggles with thin surface reconstruction and multi-view consistency, **2D Gaussian Splatting (2DGS)** addresses these limitations through a more surface-aligned formulation. This method enables high-fidelity geometry and photorealistic view synthesis from sparse input, making it a promising technique for applications such as novel view rendering, scene reconstruction, and AR/VR content creation.
---

## Paper Info

- **Title**: 2D Gaussian Splatting for Geometrically Accurate Radiance Fields
- **Authors**: Binbin Huang, Zehao Yu2, Anpei Chen, Andreas Geiger, Shenghua Gao
- **Conference**: SIGGRAPH 2024 
- **Paper**: [2DGS](https://arxiv.org/abs/2403.17888)

## 3. 3D Gaussian Splatting Overview

To fully appreciate the improvements brought by **2D Gaussian Splatting**, it's helpful to understand the foundation laid by **3D Gaussian Splatting (3DGS)**. If you're unfamiliar with 3DGS or want a refresher, check out my detailed technical review here:

üëâ [Understanding 3D Gaussian Splatting: A Technical Overview](https://jiwonhaha.github.io/jtech-blog/paper%20review/3DGS/)

In short, 3DGS represents scenes using volumetric 3D Gaussian primitives and renders them via differentiable splatting. While effective for view synthesis, it faces challenges with thin surface reconstruction, normal estimation, and multi-view consistency ‚Äî all of which 2DGS aims to overcome.


---
## 4. 2D Gaussian Splatting

To better reconstruct geometry while keeping the rendering photorealistic, this paper introduces **2D Gaussian Splatting (2DGS)**.

![2DGS]({{ site.baseurl }}/assets/images/2DGS/2DGS_explain.png)

### 4.1 Modeling

Compared to 3DGS, 2DGS simplifies things by using "flat" 2D Gaussians embedded in 3D space. Each splat lies in a local tangent plane and is defined by:

- A center point: **p‚Çñ**
- Two tangential vectors: **t·µ§**, **t·µ•**
- A scaling vector: **s = (s·µ§, s·µ•)**
- A normal vector: **tùë§ = t·µ§ √ó t·µ•**

These form a rotation matrix:

$$
R = [t_u, t_v, t_w]
$$

and a scaling matrix **S**, where the third row is zero. The transformation is defined as:

$$
P(u, v) = p_k + s_u t_u u + s_v t_v v = H(u, v, 1, 1)^T
$$

with:

$$
H = \begin{bmatrix} s_u t_u & s_v t_v & 0 & p_k \\ 0 & 0 & 0 & 1 \end{bmatrix}
$$

The 2D Gaussian value is:

$$
G(u) = \exp\left(-\frac{u^2 + v^2}{2}\right)
$$

Each splat also carries an opacity **Œ±** and a view-dependent color **c**, modeled with spherical harmonics.

---

### 4.2 Splatting

#### Affine Projection

Affine projection only works well at the center of the Gaussian and gets worse further out. To improve this, 2DGS uses homogeneous coordinates:

$$
x = (xz, yz, z, z)^T = W H(u, v, 1, 1)^T
$$

To avoid numerical issues from matrix inversion, we apply:

$$
h_u = (WH)^T h_x,\quad h_v = (WH)^T h_y
$$

Then solve for the intersection point by:

$$
h_u \cdot (u, v, 1, 1)^T = h_v \cdot (u, v, 1, 1)^T = 0
$$

Which leads to:

$$
u(x) = \frac{h_2^u h_4^v - h_4^u h_2^v}{h_1^u h_2^v - h_2^u h_1^v},\quad 
v(x) = \frac{h_1^u h_4^v - h_4^u h_1^v}{h_1^u h_2^v - h_2^u h_1^v}
$$

#### Degenerate Views

When a splat is viewed from the side, it can collapse into a line and be missed in rasterization. To avoid this, a low-pass filter is applied:

$$
\hat{G}(x) = \max(G(u(x)), G(x - c, \sigma))
$$

with **œÉ = ‚àö2 / 2**, which ensures enough coverage even in edge cases.

---

### Rasterization

Rasterization works like in 3DGS:

1. Compute screen-space bounding boxes
2. Sort Gaussians by depth
3. Tile them into screen bins
4. Apply volumetric alpha blending:

$$
c(x) = \sum_i c_i \alpha_i \hat{G}_i(u(x)) \prod_{j=1}^{i-1} (1 - \alpha_j \hat{G}_j(u(x)))
$$

---

## 5. Training

Photometric loss alone isn‚Äôt enough ‚Äî it often leads to noisy results. So two extra regularization losses are added.

### Depth Distortion Loss

To encourage splats to stay tight along the ray:

$$
L_d = \sum_{i,j} \omega_i \omega_j |z_i - z_j|
$$

where:

$$
\omega_i = \alpha_i \hat{G}_i(u(x)) \prod_{j=1}^{i-1} (1 - \alpha_j \hat{G}_j(u(x)))
$$

This helps sharpen the geometry by aligning splats along the ray.

### Normal Consistency Loss

To ensure that splats are aligned with the surface, normals are compared:

$$
L_n = \sum_i \omega_i (1 - n_i^T N)
$$

Here, **n·µ¢** is the splat‚Äôs normal (facing the camera) and **N** is the surface normal, estimated with finite differences:

$$
N(x, y) = \frac{\nabla_x p \times \nabla_y p}{\|\nabla_x p \times \nabla_y p\|}
$$

---

### Final Loss

The model is trained using posed images and a sparse point cloud. The total loss is:

$$
L = L_c + \alpha L_d + \beta L_n
$$

Where:

- **L_c** is the RGB reconstruction loss (L1 + D-SSIM)
- **Œ± = 1000** for bounded scenes, **100** for unbounded
- **Œ≤ = 0.05** across the board

